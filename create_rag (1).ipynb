{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a9764f2-f4af-49ce-a527-6c4f745c111f",
   "metadata": {
    "id": "5a9764f2-f4af-49ce-a527-6c4f745c111f"
   },
   "source": [
    "## imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb284ab3-c42a-4787-815c-62bb4d7d179f",
   "metadata": {
    "id": "eb284ab3-c42a-4787-815c-62bb4d7d179f"
   },
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import PyPDFLoader, PyMuPDFLoader\n",
    "# from langchain_community.document_loaders import Docx2txtLoader\n",
    "import os\n",
    "import json\n",
    "\n",
    "from langchain_community.retrievers import BM25Retriever\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.docstore.document import Document\n",
    "\n",
    "import requests\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "33955a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "folder_path = \"Inputs GenAI BMS\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "M5xpfeHD6Dzk",
   "metadata": {
    "id": "M5xpfeHD6Dzk"
   },
   "source": [
    "## helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "821726e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "88df440c-7f82-489b-a445-fe0aca9a072c",
   "metadata": {
    "id": "88df440c-7f82-489b-a445-fe0aca9a072c",
    "outputId": "7f37ccaa-87c4-4b20-ac2e-7773b88234da"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📂 43 fichiers trouvés dans Inputs GenAI BMS\n",
      "📄 Extraction PDF : 140228_Ladestrategie_und_Regelungstechnik_Schulung.pdf\n",
      "❌ Erreur OCR PDF (Inputs GenAI BMS\\140228_Ladestrategie_und_Regelungstechnik_Schulung.pdf): /usr/bin/tesseract is not installed or it's not in your PATH. See README file for more information.\n",
      "📄 Extraction PDF : applsci-12-10756-v3.pdf\n",
      "❌ Erreur OCR PDF (Inputs GenAI BMS\\applsci-12-10756-v3.pdf): /usr/bin/tesseract is not installed or it's not in your PATH. See README file for more information.\n",
      "📄 Extraction PDF : Arrow-Infineon-Battery-Management-Systems-BMS Whitepaper.pdf\n",
      "❌ Erreur OCR PDF (Inputs GenAI BMS\\Arrow-Infineon-Battery-Management-Systems-BMS Whitepaper.pdf): /usr/bin/tesseract is not installed or it's not in your PATH. See README file for more information.\n",
      "📊 Extraction PPTX : Bit+Segmentation.pptx\n",
      "📊 Extraction PPTX : Bus+Arbitration.pptx\n",
      "📊 Extraction PPTX : CAN+Bit+Stuffing.pptx\n",
      "📊 Extraction PPTX : CAN+Error+States.pptx\n",
      "📊 Extraction PPTX : CAN+Errors.pptx\n",
      "📊 Extraction PPTX : CAN+FD+Frame+format.pptx\n",
      "📊 Extraction PPTX : CAN+FD+INtroduction.pptx\n",
      "📊 Extraction PPTX : CAN+Features (1).pptx\n",
      "📊 Extraction PPTX : Can+Frame+Types (1).pptx\n",
      "📊 Extraction PPTX : CAN+Properties (1).pptx\n",
      "📊 Extraction PPTX : Can+Tools+Intro.pptx\n",
      "📄 Extraction PDF : Concept MIL High Level Testing.pdf\n",
      "❌ Erreur OCR PDF (Inputs GenAI BMS\\Concept MIL High Level Testing.pdf): /usr/bin/tesseract is not installed or it's not in your PATH. See README file for more information.\n",
      "📄 Extraction PDF : Cv - Mohamed Amine Boulawdhen.pdf\n",
      "❌ Erreur OCR PDF (Inputs GenAI BMS\\Cv - Mohamed Amine Boulawdhen.pdf): /usr/bin/tesseract is not installed or it's not in your PATH. See README file for more information.\n",
      "📊 Extraction PPTX : E-Sys_flashing process.pptx\n",
      "📊 Extraction PPTX : ECU_Test project creation.pptx\n",
      "📊 Extraction PPTX : Extended+Frame+Format.pptx\n",
      "📄 Extraction PDF : Fit4HV_Speicher_Version_2021.pdf\n",
      "❌ Erreur OCR PDF (Inputs GenAI BMS\\Fit4HV_Speicher_Version_2021.pdf): /usr/bin/tesseract is not installed or it's not in your PATH. See README file for more information.\n",
      "📄 Extraction PDF : GrundlagenElektrotechnik_35006.pdf\n",
      "❌ Erreur OCR PDF (Inputs GenAI BMS\\GrundlagenElektrotechnik_35006.pdf): /usr/bin/tesseract is not installed or it's not in your PATH. See README file for more information.\n",
      "📄 Extraction PDF : How2CANalyzer.pdf\n",
      "❌ Erreur OCR PDF (Inputs GenAI BMS\\How2CANalyzer.pdf): /usr/bin/tesseract is not installed or it's not in your PATH. See README file for more information.\n",
      "📄 Extraction PDF : How2Controldesk2_Gen5.pdf\n",
      "❌ Erreur OCR PDF (Inputs GenAI BMS\\How2Controldesk2_Gen5.pdf): /usr/bin/tesseract is not installed or it's not in your PATH. See README file for more information.\n",
      "📄 Extraction PDF : How2Ediabas.pdf\n",
      "❌ Erreur OCR PDF (Inputs GenAI BMS\\How2Ediabas.pdf): /usr/bin/tesseract is not installed or it's not in your PATH. See README file for more information.\n",
      "📄 Extraction PDF : How2INCA.pdf\n",
      "❌ Erreur OCR PDF (Inputs GenAI BMS\\How2INCA.pdf): /usr/bin/tesseract is not installed or it's not in your PATH. See README file for more information.\n",
      "📊 Extraction PPTX : How2Inca_WIP_JB.pptx\n",
      "📄 Extraction PDF : How2JIRA.pdf\n",
      "❌ Erreur OCR PDF (Inputs GenAI BMS\\How2JIRA.pdf): /usr/bin/tesseract is not installed or it's not in your PATH. See README file for more information.\n",
      "📄 Extraction PDF : Infineon-INF1197_ART_BMS_Whitepaper_d08-Whitepaper-v01_00-EN.pdf\n",
      "❌ Erreur OCR PDF (Inputs GenAI BMS\\Infineon-INF1197_ART_BMS_Whitepaper_d08-Whitepaper-v01_00-EN.pdf): /usr/bin/tesseract is not installed or it's not in your PATH. See README file for more information.\n",
      "📊 Extraction PPTX : L4_Report_mit_Iso_Coverage.pptx\n",
      "📊 Extraction PPTX : Package_mit_Jenkins_ausfuehren.pptx\n",
      "📊 Extraction PPTX : schulung_Tracanalyse_27_06.pptx\n",
      "📄 Extraction PDF : Short_ISTQB.pdf\n",
      "❌ Erreur OCR PDF (Inputs GenAI BMS\\Short_ISTQB.pdf): /usr/bin/tesseract is not installed or it's not in your PATH. See README file for more information.\n",
      "📄 Extraction PDF : sustainability-14-15912.pdf\n",
      "❌ Erreur OCR PDF (Inputs GenAI BMS\\sustainability-14-15912.pdf): /usr/bin/tesseract is not installed or it's not in your PATH. See README file for more information.\n",
      "📊 Extraction PPTX : What+is+CAN.pptx\n",
      "📊 Extraction PPTX : Zusammenfassung Flaschen mit E-sys 3.35.1. und Rolle beantragen (002).pptx\n",
      "✅ Extraction terminée ! Données enregistrées dans extracted_dataset.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import os\n",
    "import pdfplumber  # Extraction texte des PDFs\n",
    "import pytesseract  # OCR pour images et PDFs scannés\n",
    "import cv2\n",
    "import pandas as pd\n",
    "from pdf2image import convert_from_path  # Convertir PDF en images\n",
    "from pptx import Presentation  # Extraction texte PowerPoint\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "# Configuration\n",
    "folder_path = \"Inputs GenAI BMS\"  # Modifier avec votre chemin réel\n",
    "output_csv = \"extracted_dataset.csv\"\n",
    "pytesseract.pytesseract.tesseract_cmd = r\"/usr/bin/tesseract\"  # Modifier selon installation\n",
    "\n",
    "# Liste des données extraites\n",
    "dataset = []\n",
    "\n",
    "# Vérifier si le dossier existe\n",
    "if not os.path.exists(folder_path):\n",
    "    print(f\"❌ Dossier introuvable : {folder_path}\")\n",
    "    exit()\n",
    "\n",
    "# Vérifier le nombre de fichiers trouvés\n",
    "files = os.listdir(folder_path)\n",
    "if not files:\n",
    "    print(\"❌ Aucun fichier trouvé dans le dossier.\")\n",
    "    exit()\n",
    "print(f\"📂 {len(files)} fichiers trouvés dans {folder_path}\")\n",
    "\n",
    "# Fonction pour extraire du texte depuis un PDF avec pdfplumber\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    text_pages = []\n",
    "    try:\n",
    "        with pdfplumber.open(pdf_path) as pdf:\n",
    "            for page_num, page in enumerate(pdf.pages, start=1):\n",
    "                text = page.extract_text() or \"\"  # Récupérer le texte ou une chaîne vide\n",
    "                text_pages.append({\"page_num\": page_num, \"text\": text.strip()})\n",
    "        return text_pages\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Erreur extraction PDF ({pdf_path}): {e}\")\n",
    "        return []\n",
    "\n",
    "# Fonction OCR sur images d'un PDF\n",
    "def extract_ocr_from_pdf(pdf_path, dpi=150, max_pages=5):\n",
    "    ocr_text = []\n",
    "    try:\n",
    "        images = convert_from_path(pdf_path, dpi=dpi, first_page=1, last_page=max_pages)\n",
    "        for img_num, img in enumerate(images, start=1):\n",
    "            img_array = cv2.cvtColor(np.array(img), cv2.COLOR_RGB2BGR)\n",
    "            h, w, _ = img_array.shape\n",
    "            if w < 50 or h < 50:\n",
    "                continue  # Ignorer les petites images\n",
    "\n",
    "            gray = cv2.cvtColor(img_array, cv2.COLOR_BGR2GRAY)\n",
    "            gray = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1]\n",
    "            text = pytesseract.image_to_string(gray, lang=\"fra+eng\").strip()\n",
    "            if text:\n",
    "                ocr_text.append({\"page_num\": img_num, \"ocr_text\": text})\n",
    "        return ocr_text\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Erreur OCR PDF ({pdf_path}): {e}\")\n",
    "        return []\n",
    "\n",
    "# Fonction pour extraire le texte d'un PowerPoint\n",
    "def extract_text_from_pptx(pptx_path):\n",
    "    text = []\n",
    "    try:\n",
    "        prs = Presentation(pptx_path)\n",
    "        for slide_num, slide in enumerate(prs.slides, start=1):\n",
    "            slide_text = \"\\n\".join(shape.text.strip() for shape in slide.shapes if hasattr(shape, \"text\"))\n",
    "            text.append({\"slide_num\": slide_num, \"text\": slide_text.strip()})\n",
    "        return text\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Erreur extraction PPTX ({pptx_path}): {e}\")\n",
    "        return []\n",
    "\n",
    "# Traitement des fichiers\n",
    "df_data = []\n",
    "for file_name in files:\n",
    "    file_path = os.path.join(folder_path, file_name)\n",
    "    \n",
    "    if file_name.endswith(\".pdf\"):\n",
    "        print(f\"📄 Extraction PDF : {file_name}\")\n",
    "        pdf_text = extract_text_from_pdf(file_path)\n",
    "        pdf_ocr = extract_ocr_from_pdf(file_path, dpi=150, max_pages=5)\n",
    "\n",
    "        for page in pdf_text:\n",
    "            page_num = page[\"page_num\"]\n",
    "            ocr_text = next((ocr[\"ocr_text\"] for ocr in pdf_ocr if ocr[\"page_num\"] == page_num), \"\")\n",
    "            df_data.append({\"file\": file_name, \"text\": page[\"text\"], \"ocr_text\": ocr_text})\n",
    "\n",
    "    elif file_name.endswith(\".pptx\"):\n",
    "        print(f\"📊 Extraction PPTX : {file_name}\")\n",
    "        ppt_text = extract_text_from_pptx(file_path)\n",
    "        for slide in ppt_text:\n",
    "            df_data.append({\"file\": file_name, \"text\": slide[\"text\"], \"ocr_text\": \"\"})\n",
    "\n",
    "# Vérifier si des données ont été extraites\n",
    "if df_data:\n",
    "    df = pd.DataFrame(df_data)\n",
    "    df.to_csv(output_csv, index=False, encoding=\"utf-8\")\n",
    "    print(f\"✅ Extraction terminée ! Données enregistrées dans {output_csv}\")\n",
    "else:\n",
    "    print(\"❌ Aucune donnée extraite, le fichier CSV ne sera pas généré.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ad25fe38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data cleaned. Number of rows after cleaning: 770\n",
      "Cleaned dataset saved to cleaned_dataset.csv\n"
     ]
    }
   ],
   "source": [
    "#nettoyage des données\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Charger le dataset extrait\n",
    "df = pd.read_csv(\"extracted_dataset.csv\", encoding=\"utf-8\")\n",
    "\n",
    "# Nettoyage des colonnes \"text\" et \"ocr_text\"\n",
    "def clean_text(text):\n",
    "    # Conversion en minuscules\n",
    "    text = text.lower()\n",
    "\n",
    "    #Suppression des espaces superflus\n",
    "    text = text.strip()\n",
    "\n",
    "    #Suppression des caractères spéciaux et des retours à la ligne inutiles\n",
    "    text = re.sub(r'\\n+', ' ', text)  # Remplacer les sauts de ligne par des espaces\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # Supprimer tous les caractères spéciaux\n",
    "\n",
    "    #Suppression des multiples espaces\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "\n",
    "    return text\n",
    "\n",
    "#Appliquer le nettoyage à la colonne \"text\" et \"ocr_text\"\n",
    "df['text'] = df['text'].apply(lambda x: clean_text(str(x)))\n",
    "df['ocr_text'] = df['ocr_text'].apply(lambda x: clean_text(str(x)))\n",
    "\n",
    "#Suppression des doublons (lignes identiques)\n",
    "df = df.drop_duplicates(subset=[\"text\", \"ocr_text\"])\n",
    "\n",
    "#Suppression des lignes avec des valeurs manquantes\n",
    "df = df.dropna(subset=[\"text\", \"ocr_text\"])\n",
    "\n",
    "#Vérification après nettoyage\n",
    "print(f\"Data cleaned. Number of rows after cleaning: {len(df)}\")\n",
    "\n",
    "#Sauvegarder les données nettoyées dans un nouveau fichier CSV\n",
    "df.to_csv(\"cleaned_dataset.csv\", index=False, encoding=\"utf-8\")\n",
    "print(f\"Cleaned dataset saved to cleaned_dataset.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a81cAg6GjM",
   "metadata": {
    "id": "70a81cAg6GjM"
   },
   "source": [
    "# Chunking method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8b450645-fcb1-473f-97f4-1c10096ab437",
   "metadata": {
    "id": "8b450645-fcb1-473f-97f4-1c10096ab437"
   },
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=800,\n",
    "    chunk_overlap=300,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "uNCzqqFM6L8j",
   "metadata": {
    "id": "uNCzqqFM6L8j"
   },
   "source": [
    "# chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d5d119e6-a5b7-469f-b75c-f29c51ebc2a5",
   "metadata": {
    "id": "d5d119e6-a5b7-469f-b75c-f29c51ebc2a5"
   },
   "outputs": [],
   "source": [
    "chunks_all = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8f9e0802-9693-45a2-8c6b-a9fef3a183e3",
   "metadata": {
    "id": "8f9e0802-9693-45a2-8c6b-a9fef3a183e3",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "def split_dataset_into_chunks(df, chunk_size=800, chunk_overlap=200):\n",
    "    \"\"\"Divise le dataset en chunks en utilisant CharacterTextSplitter.\n",
    "\n",
    "    Args:\n",
    "        df: Le DataFrame Pandas contenant les données à diviser.\n",
    "        chunk_size: La taille maximale de chaque chunk en caractères.\n",
    "        chunk_overlap: Le chevauchement entre les chunks en caractères.\n",
    "\n",
    "    Returns:\n",
    "        Une liste de dictionnaires, où chaque dictionnaire représente un chunk\n",
    "        et contient les clés \"chunk\", \"file\" et \"type\".\n",
    "    \"\"\"\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap\n",
    "    )\n",
    "\n",
    "    chunks_all = []\n",
    "    for index, row in df.iterrows():\n",
    "        chunks = text_splitter.split_text(row['text'])\n",
    "        for chunk in chunks:\n",
    "            chunks_all.append({\n",
    "                'chunk': chunk,\n",
    "                'file': row['file'],\n",
    "                'type': 'text'  # Indique que le chunk provient de la colonne 'text'\n",
    "            })\n",
    "\n",
    "        # Faire de même pour la colonne 'ocr_text' si nécessaire\n",
    "        chunks = text_splitter.split_text(row['ocr_text'])\n",
    "        for chunk in chunks:\n",
    "            chunks_all.append({\n",
    "                'chunk': chunk,\n",
    "                'file': row['file'],\n",
    "                'type': 'ocr_text'  # Indique que le chunk provient de la colonne 'ocr_text'\n",
    "            })\n",
    "    \n",
    "    return chunks_all\n",
    "\n",
    "# Utilisation de la fonction :\n",
    "chunks_all = split_dataset_into_chunks(df) \n",
    "\n",
    "# Enregistrement des chunks dans un fichier CSV si vous le souhaitez :\n",
    "import pandas as pd\n",
    "chunks_df = pd.DataFrame(chunks_all)\n",
    "chunks_df.to_csv(\"chunks_dataset.csv\", index=False, encoding=\"utf-8\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eKdJjB5c5W2-",
   "metadata": {
    "id": "eKdJjB5c5W2-"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2020"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(chunks_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a67de0f8-7ed1-43bd-b248-c1ede30f564a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sysatc\\anaconda3\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:151: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 0.3.0. An updated version of the class exists in the langchain-huggingface package and should be used instead. To use it run `pip install -U langchain-huggingface` and import as `from langchain_huggingface import HuggingFaceEmbeddings`.\n",
      "  warn_deprecated(\n",
      "c:\\Users\\sysatc\\anaconda3\\Lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\sysatc\\anaconda3\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sysatc\\anaconda3\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "model_kwargs = {\"trust_remote_code\": True, \"device\": \"cpu\"}\n",
    "embedding_function = HuggingFaceEmbeddings(model_name=\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\", model_kwargs=model_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8311cf7d-df98-4294-b47c-0afc22041564",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: chromadb in c:\\users\\sysatc\\anaconda3\\lib\\site-packages (1.0.5)\n",
      "Requirement already satisfied: build>=1.0.3 in c:\\users\\sysatc\\anaconda3\\lib\\site-packages (from chromadb) (1.2.2.post1)\n",
      "Requirement already satisfied: pydantic>=1.9 in c:\\users\\sysatc\\anaconda3\\lib\\site-packages (from chromadb) (2.8.2)\n",
      "Requirement already satisfied: chroma-hnswlib==0.7.6 in c:\\users\\sysatc\\anaconda3\\lib\\site-packages (from chromadb) (0.7.6)\n",
      "Requirement already satisfied: fastapi==0.115.9 in c:\\users\\sysatc\\anaconda3\\lib\\site-packages (from chromadb) (0.115.9)\n",
      "Requirement already satisfied: uvicorn>=0.18.3 in c:\\users\\sysatc\\anaconda3\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.34.0)\n",
      "Requirement already satisfied: numpy>=1.22.5 in c:\\users\\sysatc\\anaconda3\\lib\\site-packages (from chromadb) (1.26.4)\n",
      "Requirement already satisfied: posthog>=2.4.0 in c:\\users\\sysatc\\anaconda3\\lib\\site-packages (from chromadb) (3.25.0)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in c:\\users\\sysatc\\anaconda3\\lib\\site-packages (from chromadb) (4.11.0)\n",
      "Requirement already satisfied: onnxruntime>=1.14.1 in c:\\users\\sysatc\\anaconda3\\lib\\site-packages (from chromadb) (1.21.0)\n",
      "Requirement already satisfied: opentelemetry-api>=1.2.0 in c:\\users\\sysatc\\anaconda3\\lib\\site-packages (from chromadb) (1.32.1)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in c:\\users\\sysatc\\anaconda3\\lib\\site-packages (from chromadb) (1.32.1)\n",
      "Requirement already satisfied: opentelemetry-instrumentation-fastapi>=0.41b0 in c:\\users\\sysatc\\anaconda3\\lib\\site-packages (from chromadb) (0.53b1)\n",
      "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in c:\\users\\sysatc\\anaconda3\\lib\\site-packages (from chromadb) (1.32.1)\n",
      "Requirement already satisfied: tokenizers>=0.13.2 in c:\\users\\sysatc\\anaconda3\\lib\\site-packages (from chromadb) (0.19.1)\n",
      "Requirement already satisfied: pypika>=0.48.9 in c:\\users\\sysatc\\anaconda3\\lib\\site-packages (from chromadb) (0.48.9)\n",
      "Requirement already satisfied: tqdm>=4.65.0 in c:\\users\\sysatc\\anaconda3\\lib\\site-packages (from chromadb) (4.66.4)\n",
      "Requirement already satisfied: overrides>=7.3.1 in c:\\users\\sysatc\\anaconda3\\lib\\site-packages (from chromadb) (7.4.0)\n",
      "Requirement already satisfied: importlib-resources in c:\\users\\sysatc\\anaconda3\\lib\\site-packages (from chromadb) (6.5.2)\n",
      "Requirement already satisfied: grpcio>=1.58.0 in c:\\users\\sysatc\\anaconda3\\lib\\site-packages (from chromadb) (1.70.0)\n",
      "Requirement already satisfied: bcrypt>=4.0.1 in c:\\users\\sysatc\\anaconda3\\lib\\site-packages (from chromadb) (4.3.0)\n",
      "Requirement already satisfied: typer>=0.9.0 in c:\\users\\sysatc\\anaconda3\\lib\\site-packages (from chromadb) (0.15.1)\n",
      "Requirement already satisfied: kubernetes>=28.1.0 in c:\\users\\sysatc\\anaconda3\\lib\\site-packages (from chromadb) (32.0.1)\n",
      "Requirement already satisfied: tenacity>=8.2.3 in c:\\users\\sysatc\\anaconda3\\lib\\site-packages (from chromadb) (9.1.2)\n",
      "Requirement already satisfied: pyyaml>=6.0.0 in c:\\users\\sysatc\\anaconda3\\lib\\site-packages (from chromadb) (6.0.1)\n",
      "Requirement already satisfied: mmh3>=4.0.1 in c:\\users\\sysatc\\anaconda3\\lib\\site-packages (from chromadb) (5.1.0)\n",
      "Requirement already satisfied: orjson>=3.9.12 in c:\\users\\sysatc\\anaconda3\\lib\\site-packages (from chromadb) (3.10.7)\n",
      "Requirement already satisfied: httpx>=0.27.0 in c:\\users\\sysatc\\anaconda3\\lib\\site-packages (from chromadb) (0.27.0)\n",
      "Requirement already satisfied: rich>=10.11.0 in c:\\users\\sysatc\\anaconda3\\lib\\site-packages (from chromadb) (13.3.5)\n",
      "Requirement already satisfied: jsonschema>=4.19.0 in c:\\users\\sysatc\\anaconda3\\lib\\site-packages (from chromadb) (4.19.2)\n",
      "Requirement already satisfied: starlette<0.46.0,>=0.40.0 in c:\\users\\sysatc\\anaconda3\\lib\\site-packages (from fastapi==0.115.9->chromadb) (0.45.3)\n",
      "Requirement already satisfied: packaging>=19.1 in c:\\users\\sysatc\\anaconda3\\lib\\site-packages (from build>=1.0.3->chromadb) (23.2)\n",
      "Requirement already satisfied: pyproject_hooks in c:\\users\\sysatc\\anaconda3\\lib\\site-packages (from build>=1.0.3->chromadb) (1.2.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\sysatc\\anaconda3\\lib\\site-packages (from build>=1.0.3->chromadb) (0.4.6)\n",
      "Requirement already satisfied: anyio in c:\\users\\sysatc\\anaconda3\\lib\\site-packages (from httpx>=0.27.0->chromadb) (4.2.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\sysatc\\anaconda3\\lib\\site-packages (from httpx>=0.27.0->chromadb) (2024.6.2)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\sysatc\\anaconda3\\lib\\site-packages (from httpx>=0.27.0->chromadb) (1.0.5)\n",
      "Requirement already satisfied: idna in c:\\users\\sysatc\\anaconda3\\lib\\site-packages (from httpx>=0.27.0->chromadb) (3.7)\n",
      "Requirement already satisfied: sniffio in c:\\users\\sysatc\\anaconda3\\lib\\site-packages (from httpx>=0.27.0->chromadb) (1.3.0)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\sysatc\\anaconda3\\lib\\site-packages (from httpcore==1.*->httpx>=0.27.0->chromadb) (0.14.0)\n",
      "Requirement already satisfied: attrs>=22.2.0 in c:\\users\\sysatc\\anaconda3\\lib\\site-packages (from jsonschema>=4.19.0->chromadb) (23.1.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in c:\\users\\sysatc\\anaconda3\\lib\\site-packages (from jsonschema>=4.19.0->chromadb) (2023.7.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in c:\\users\\sysatc\\anaconda3\\lib\\site-packages (from jsonschema>=4.19.0->chromadb) (0.30.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in c:\\users\\sysatc\\anaconda3\\lib\\site-packages (from jsonschema>=4.19.0->chromadb) (0.10.6)\n",
      "Requirement already satisfied: six>=1.9.0 in c:\\users\\sysatc\\anaconda3\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (1.16.0)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in c:\\users\\sysatc\\anaconda3\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (2.9.0.post0)\n",
      "Requirement already satisfied: google-auth>=1.0.1 in c:\\users\\sysatc\\anaconda3\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (2.39.0)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in c:\\users\\sysatc\\anaconda3\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (1.8.0)\n",
      "Requirement already satisfied: requests in c:\\users\\sysatc\\anaconda3\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (2.32.2)\n",
      "Requirement already satisfied: requests-oauthlib in c:\\users\\sysatc\\anaconda3\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (2.0.0)\n",
      "Requirement already satisfied: oauthlib>=3.2.2 in c:\\users\\sysatc\\anaconda3\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (3.2.2)\n",
      "Requirement already satisfied: urllib3>=1.24.2 in c:\\users\\sysatc\\anaconda3\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (2.2.2)\n",
      "Requirement already satisfied: durationpy>=0.7 in c:\\users\\sysatc\\anaconda3\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (0.9)\n",
      "Requirement already satisfied: coloredlogs in c:\\users\\sysatc\\anaconda3\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in c:\\users\\sysatc\\anaconda3\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb) (25.1.24)\n",
      "Requirement already satisfied: protobuf in c:\\users\\sysatc\\anaconda3\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb) (5.29.4)\n",
      "Requirement already satisfied: sympy in c:\\users\\sysatc\\anaconda3\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb) (1.12)\n",
      "Requirement already satisfied: deprecated>=1.2.6 in c:\\users\\sysatc\\anaconda3\\lib\\site-packages (from opentelemetry-api>=1.2.0->chromadb) (1.2.18)\n",
      "Requirement already satisfied: importlib-metadata<8.7.0,>=6.0 in c:\\users\\sysatc\\anaconda3\\lib\\site-packages (from opentelemetry-api>=1.2.0->chromadb) (7.0.1)\n",
      "Requirement already satisfied: googleapis-common-protos~=1.52 in c:\\users\\sysatc\\anaconda3\\lib\\site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.70.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.32.1 in c:\\users\\sysatc\\anaconda3\\lib\\site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.32.1)\n",
      "Requirement already satisfied: opentelemetry-proto==1.32.1 in c:\\users\\sysatc\\anaconda3\\lib\\site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.32.1)\n",
      "Requirement already satisfied: opentelemetry-instrumentation-asgi==0.53b1 in c:\\users\\sysatc\\anaconda3\\lib\\site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.53b1)\n",
      "Requirement already satisfied: opentelemetry-instrumentation==0.53b1 in c:\\users\\sysatc\\anaconda3\\lib\\site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.53b1)\n",
      "Requirement already satisfied: opentelemetry-semantic-conventions==0.53b1 in c:\\users\\sysatc\\anaconda3\\lib\\site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.53b1)\n",
      "Requirement already satisfied: opentelemetry-util-http==0.53b1 in c:\\users\\sysatc\\anaconda3\\lib\\site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.53b1)\n",
      "Requirement already satisfied: wrapt<2.0.0,>=1.0.0 in c:\\users\\sysatc\\anaconda3\\lib\\site-packages (from opentelemetry-instrumentation==0.53b1->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (1.14.1)\n",
      "Requirement already satisfied: asgiref~=3.0 in c:\\users\\sysatc\\anaconda3\\lib\\site-packages (from opentelemetry-instrumentation-asgi==0.53b1->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (3.8.1)\n",
      "Requirement already satisfied: monotonic>=1.5 in c:\\users\\sysatc\\anaconda3\\lib\\site-packages (from posthog>=2.4.0->chromadb) (1.6)\n",
      "Requirement already satisfied: backoff>=1.10.0 in c:\\users\\sysatc\\anaconda3\\lib\\site-packages (from posthog>=2.4.0->chromadb) (2.2.1)\n",
      "Requirement already satisfied: distro>=1.5.0 in c:\\users\\sysatc\\anaconda3\\lib\\site-packages (from posthog>=2.4.0->chromadb) (1.9.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\sysatc\\anaconda3\\lib\\site-packages (from pydantic>=1.9->chromadb) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in c:\\users\\sysatc\\anaconda3\\lib\\site-packages (from pydantic>=1.9->chromadb) (2.20.1)\n",
      "Requirement already satisfied: markdown-it-py<3.0.0,>=2.2.0 in c:\\users\\sysatc\\anaconda3\\lib\\site-packages (from rich>=10.11.0->chromadb) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\sysatc\\anaconda3\\lib\\site-packages (from rich>=10.11.0->chromadb) (2.15.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in c:\\users\\sysatc\\anaconda3\\lib\\site-packages (from tokenizers>=0.13.2->chromadb) (0.29.0)\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\users\\sysatc\\anaconda3\\lib\\site-packages (from typer>=0.9.0->chromadb) (8.1.7)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in c:\\users\\sysatc\\anaconda3\\lib\\site-packages (from typer>=0.9.0->chromadb) (1.5.4)\n",
      "Requirement already satisfied: httptools>=0.6.3 in c:\\users\\sysatc\\anaconda3\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.6.4)\n",
      "Requirement already satisfied: python-dotenv>=0.13 in c:\\users\\sysatc\\anaconda3\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.21.0)\n",
      "Requirement already satisfied: watchfiles>=0.13 in c:\\users\\sysatc\\anaconda3\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.0.5)\n",
      "Requirement already satisfied: websockets>=10.4 in c:\\users\\sysatc\\anaconda3\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (14.2)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\sysatc\\anaconda3\\lib\\site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (5.3.3)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\sysatc\\anaconda3\\lib\\site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\sysatc\\anaconda3\\lib\\site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\sysatc\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\sysatc\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (2024.3.1)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\sysatc\\anaconda3\\lib\\site-packages (from importlib-metadata<8.7.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.17.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\sysatc\\anaconda3\\lib\\site-packages (from markdown-it-py<3.0.0,>=2.2.0->rich>=10.11.0->chromadb) (0.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\sysatc\\anaconda3\\lib\\site-packages (from requests->kubernetes>=28.1.0->chromadb) (2.0.4)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in c:\\users\\sysatc\\anaconda3\\lib\\site-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb) (10.0)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\sysatc\\anaconda3\\lib\\site-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
      "Requirement already satisfied: pyreadline3 in c:\\users\\sysatc\\anaconda3\\lib\\site-packages (from humanfriendly>=9.1->coloredlogs->onnxruntime>=1.14.1->chromadb) (3.5.4)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\sysatc\\anaconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.4.8)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install chromadb\n",
    "\n",
    "from chromadb import Client\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b88fd3c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total chunks: 2020\n",
      "Sample chunk: {'chunk': 'schulungseminar zum einstieg in die emobility ladestrategie und regelungstechnik ort datum trainer kubilay canaltay mitgearbeitet an der entwicklung des trainings haben loßin stefan claas kubilay canaltay f 0841 d r01 201207 entwickelt für schulungszyklus emobility 2013', 'file': '140228_Ladestrategie_und_Regelungstechnik_Schulung.pdf', 'type': 'text'}\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total chunks: {len(chunks_all)}\")\n",
    "print(\"Sample chunk:\", chunks_all[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "00bdad66-0bd6-44f9-8ba0-fce8bfdcb4e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "db_4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sysatc\\anaconda3\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:439: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.schema import Document\n",
    "\n",
    "# Define the embedding function\n",
    "embedding_function = HuggingFaceEmbeddings(model_name=\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\")\n",
    "\n",
    "# Create a folder to store the database\n",
    "DB_FOLDER = f'db_4'\n",
    "print(DB_FOLDER)\n",
    "\n",
    "# Convert the chunks_all dictionary into a list of Documents\n",
    "documents = []\n",
    "\n",
    "for chunk in chunks_all:\n",
    "    metadata = {\n",
    "        'file': chunk['file'],\n",
    "        'type': chunk['type'],\n",
    "    }\n",
    "    documents.append(Document(page_content=chunk.get('chunk', ''), metadata=metadata))\n",
    "# Initialize Chroma with documents and embeddings\n",
    "db = Chroma.from_documents(documents, embedding_function, persist_directory=DB_FOLDER)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c568564a-e795-4cd9-91e6-e8fbe557aae8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f96d88c-707d-440a-b45d-fc0daabdae9e",
   "metadata": {
    "id": "4f96d88c-707d-440a-b45d-fc0daabdae9e",
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf1b74e-5358-45e0-a7c3-7ffd1dcb7add",
   "metadata": {
    "id": "cdf1b74e-5358-45e0-a7c3-7ffd1dcb7add",
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "-fWFZ8-b6RSX",
   "metadata": {
    "id": "-fWFZ8-b6RSX"
   },
   "source": [
    "# embedding and loading into the main vectorDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a56e1364-462a-4adf-a39a-566e8ba2718c",
   "metadata": {
    "id": "a56e1364-462a-4adf-a39a-566e8ba2718c",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "model_kwargs = {\"trust_remote_code\": True, \"device\": \"cpu\"}\n",
    "embedding_function = HuggingFaceEmbeddings(model_name=\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\", model_kwargs=model_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e85bc7ce-b4a2-46a2-af06-af60a8740df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "db_n = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f486c2d5-b33f-4b18-b122-60e47237a109",
   "metadata": {
    "id": "f486c2d5-b33f-4b18-b122-60e47237a109"
   },
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "db_n = db_n + 1\n",
    "DB_FOLDER = f'db_4'\n",
    "print(DB_FOLDER)\n",
    "db = Chroma.from_documents(chunks_all, embedding_function, persist_directory=DB_FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "drtxTp1G-oIp",
   "metadata": {
    "id": "drtxTp1G-oIp"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "O_ViN6BH-oLO",
   "metadata": {
    "id": "O_ViN6BH-oLO"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
